{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de GOOGLENET.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhBL8-tjHzJC",
        "colab_type": "code",
        "outputId": "a9a5ba30-656c-4604-ec19-cf8b04c5277d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        }
      },
      "source": [
        "!pip install --upgrade tensorflow-gpu==1.14\n",
        "!pip install --upgrade fire"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 143kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (3.7.1)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow-gpu==1.14)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.1.7)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow-gpu==1.14)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 38.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.16.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.14) (41.2.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14) (3.1.1)\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
            "Collecting fire\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from fire) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor in /usr/local/lib/python3.6/dist-packages (from fire) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=055a0dea2095aceb46d788118c5d3cf5178a1bb53598d8238e5142e4f8a5494a\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsiEYcm6ayiV",
        "colab_type": "code",
        "outputId": "5bdf6961-6975-44fd-edfc-5f4b2ddb9eda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "# Opens the project folder. IMPORTANT: Change to your route\n",
        "%cd 'drive/My Drive/TEC/2019_2S/IA/TareaProgramada2'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/TEC/2019_2S/IA/TareaProgramada2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85GiDiJB5DVe",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbPXW5EuISx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import fire\n",
        "import numpy as np\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
        "tf.logging.set_verbosity(tf.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ83YlGd5EIj",
        "colab_type": "code",
        "outputId": "d117993d-5446-4f81-9c44-6c56bd160695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6TSGfxoODVs",
        "colab_type": "code",
        "outputId": "59571961-de97-4039-89fa-d1edab2c9486",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f-SPW8dCvSB",
        "colab_type": "code",
        "outputId": "d5eaf2b4-c5ec-42fc-806b-8c6b459c59e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print ('x_train shape:', x_train.shape)\n",
        "print ('y_train shape: ', y_train.shape)\n",
        "print ('x_test shape: ', x_test.shape)\n",
        "print ('y_test shape:', y_test.shape)\n",
        "\n",
        "print('muestras de entrenamiento: ', x_train.shape[0])\n",
        "print('muestras de test: ', x_test.shape[0])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (50000, 32, 32, 3)\n",
            "y_train shape:  (50000, 1)\n",
            "x_test shape:  (10000, 32, 32, 3)\n",
            "y_test shape: (10000, 1)\n",
            "muestras de entrenamiento:  50000\n",
            "muestras de test:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afHx4x51I47n",
        "colab_type": "code",
        "outputId": "d1e04e11-54f4-4cba-d28a-07def24a1635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "class Train:\n",
        "  _x_ = []\n",
        "  _y_ = []\n",
        "  _logits = []\n",
        "  _loss = []\n",
        "  _train_step = []\n",
        "  _merged_summary_op = []\n",
        "  _saver = []\n",
        "  _session = []\n",
        "  _writer = []\n",
        "  _is_training = []\n",
        "  _loss_val = []\n",
        "  _train_summary = []\n",
        "  _val_summary = []\n",
        "  \n",
        "  def _init_(self):\n",
        "    pass\n",
        "  \n",
        "\n",
        "  def build_graph(self):\n",
        "    self._x_ = tf.placeholder(\"float\", shape=[None, 32, 32, 3], name='X')\n",
        "    self._y_ = tf.placeholder(\"int32\", shape=[None, 10], name='Y')\n",
        "    self._is_training = tf.placeholder(tf.bool)\n",
        "\n",
        "    with tf.name_scope(\"model\") as scope:\n",
        "\n",
        "      #arquitecuta de la RED GoogleNet Inception v1\n",
        "      \n",
        "      conv1_7x7_S2 = tf.layers.conv2d(inputs=self._x_,filters=64, kernel_size=[7,7], strides=2,  padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool_3x3_S2 = tf.layers.max_pooling2d(inputs=conv1_7x7_S2, pool_size=[3,3], strides=2)\n",
        "      norm1 = tf.nn.lrn(input=maxPool_3x3_S2, bias=1.0, alpha=0.001 / 9.0, beta=0.75,name='norm1' )\n",
        "      conv_1x1_V1 = tf.layers.conv2d(norm1,filters=64, kernel_size=[1,1], strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv_3x3_S1 = tf.layers.conv2d(conv_1x1_V1,filters=192, kernel_size=[3,3], strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      norm2 = tf.nn.lrn(input=conv_3x3_S1,  bias=1.0, alpha=0.001 / 9.0, beta=0.75,name='norm2' )\n",
        "      \n",
        "      maxPool2_3x3_S2 = tf.layers.max_pooling2d(inputs=norm2, pool_size=[3,3], strides=2)\n",
        "      \n",
        "      ###############primera capa de inception 3a\n",
        "      conv3a_1x1redu3x3_S = tf.layers.conv2d(inputs=maxPool2_3x3_S2, filters=96, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv3a_1x1redu5x5_S = tf.layers.conv2d(inputs=maxPool2_3x3_S2, filters=16, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool3a_3x3_S = tf.layers.max_pooling2d(inputs=maxPool2_3x3_S2, pool_size=[3,3], strides=1)\n",
        "      \n",
        "      conv3a_1x1_S = tf.layers.conv2d(inputs=maxPool2_3x3_S2, filters=64, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv3a_3x3_S = tf.layers.conv2d(inputs=conv3a_1x1redu3x3_S, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv3a_5x5_S = tf.layers.conv2d(inputs=conv3a_1x1redu5x5_S, filters=32, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv3aM_1x1_S = tf.layers.conv2d(inputs=maxPool3a_3x3_S, filters=32, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      #################DEPTH CONCAT\n",
        "      \n",
        "      #intento de concat \n",
        "      print(\"todo bien hasta aquí 1\")\n",
        "      #print(conv3a_1x1_S.shape,conv3a_1x1_S.shape,conv3a_3x3_S.shape,conv3a_5x5_S.shape,conv3aM_1x1_S.shape)\n",
        "\n",
        "      concat1 = tf.concat([conv3a_1x1_S,conv3a_3x3_S,conv3a_5x5_S,conv3aM_1x1_S],axis=-1,name='concat1')\n",
        "      #concat1 = tf.concat([conv3a_1x1_S,conv3a_3x3_S,conv3a_5x5_S],axis=-1)\n",
        "      print(\"todo bien hasta aquí 1.1\")\n",
        "      print(concat1.shape)\n",
        "      #############################Inception 3b\n",
        "      conv3b_1x1redu3x3_S = tf.layers.conv2d(inputs=concat1, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv3b_1x1redu5x5_S = tf.layers.conv2d(inputs=concat1, filters=32, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool3b_3x3_S = tf.layers.max_pooling2d(inputs=concat1, pool_size=[3,3], strides=1)\n",
        "      \n",
        "      conv3b_1x1_S = tf.layers.conv2d(inputs=maxPool2_3x3_S2, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv3b_3x3_S = tf.layers.conv2d(inputs=conv3a_1x1redu3x3_S, filters=192, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv3b_5x5_S = tf.layers.conv2d(inputs=conv3a_1x1redu5x5_S, filters=96, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv3bM_1x1_S = tf.layers.conv2d(inputs=maxPool3a_3x3_S, filters=64, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      #################DEPTH CONCAT\n",
        "      print(\"todo bien hasta aquí 2\")\n",
        "      #hacel el concat 2\n",
        "      \n",
        "      #intento de concat \n",
        "\n",
        "      concat2 = tf.concat([conv3b_1x1_S, conv3b_3x3_S, conv3b_5x5_S, conv3bM_1x1_S],axis=1)\n",
        "\n",
        "\n",
        "\n",
        "      ############Maxpool 3\n",
        "      maxPool3_3x3_2S = tf.layers.max_pooling2d(inputs=concat2, pool_size=[3,3], strides=2)\n",
        "      \n",
        "      \n",
        "      \n",
        "      ####Inception 4a###########################\n",
        "      conv4a_1x1redu3x3_S = tf.layers.conv2d(inputs=maxPool3_3x3_2S, filters=96, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4a_1x1redu5x5_S = tf.layers.conv2d(inputs=maxPool3_3x3_2S, filters=16, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool4a_3x3_S = tf.layers.max_pooling2d(inputs=maxPool3_3x3_2S, pool_size=[3,3], strides=1)\n",
        "      \n",
        "      conv4a_1x1_S = tf.layers.conv2d(inputs=maxPool3_3x3_2S, filters=192, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4a_3x3_S = tf.layers.conv2d(inputs=conv4a_1x1redu3x3_S, filters=208, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4a_5x5_S = tf.layers.conv2d(inputs=conv4a_1x1redu5x5_S, filters=48, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4aM_1x1_S = tf.layers.conv2d(inputs=maxPool4a_3x3_S, filters=64, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      \n",
        "      print(\"todo bien hasta aquí 3\")\n",
        "      #hacer el concat 3\n",
        "      #intento de concat \n",
        "\n",
        "      concat3 = tf.concat([conv4a_1x1_S, conv4a_3x3_S, conv4a_5x5_S,conv4aM_1x1_S],axis=1)\n",
        "     \n",
        "      \n",
        "      \n",
        "      \n",
        "      #############################Inception 4b\n",
        "      conv4b_1x1redu3x3_S = tf.layers.conv2d(inputs=concat3, filters=112, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4b_1x1redu5x5_S = tf.layers.conv2d(inputs=concat3, filters=24, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool4b_3x3_S = tf.layers.max_pooling2d(inputs=concat3, pool_size=[3,3], strides=1)\n",
        "      averagePool4b_5x5_3V = tf.layers.average_pooling2d(inputs=concat3, pool_size=[5,5], strides=3)\n",
        "      \n",
        "      conv4b_1x1_S = tf.layers.conv2d(inputs=concat3, filters=160, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4b_3x3_S = tf.layers.conv2d(inputs=conv4b_1x1redu3x3_S, filters=224, kernel_size=[3,3],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4b_5x5_S = tf.layers.conv2d(inputs=conv4b_1x1redu5x5_S, filters=64, kernel_size=[5,5],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4bM_1x1_S = tf.layers.conv2d(inputs=maxPool4b_3x3_S, filters=64, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      conv4bAP_1x1_S = tf.layers.conv2d(inputs=averagePool4b_5x5_3V, filters=160, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      FC_4b1 = tf.layers.dense(inputs=conv4bAP_1x1_S, units=1024, activation=tf.nn.relu)\n",
        "      FC_4b2 = tf.layers.dense(inputs=FC_4b1, units=1000, activation=tf.nn.relu)\n",
        "      softMax1= tf.keras.activations.softmax(FC_4b2 , axis=1)\n",
        "      # aqui el concat 4\n",
        "\n",
        "      #intento de concat \n",
        "\n",
        "      concat4 = tf.concat([conv4b_1x1_S, conv4b_3x3_S,conv4b_5x5_S ,conv4bM_1x1_S],axis=1)\n",
        "      print(\"todo bien hasta aquí 4\")\n",
        "     \n",
        "      \n",
        "\n",
        "      ######inception 4c\n",
        "      conv4c_1x1redu3x3_S = tf.layers.conv2d(inputs=concat4, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4c_1x1redu5x5_S = tf.layers.conv2d(inputs=concat4, filters=24, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool4c_3x3_S = tf.layers.max_pooling2d(inputs=concat4, pool_size=[3,3], strides=1)\n",
        "      \n",
        "      conv4c_1x1_S = tf.layers.conv2d(inputs=concat4, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4c_3x3_S = tf.layers.conv2d(inputs=conv4c_1x1redu3x3_S, filters=256, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4c_5x5_S = tf.layers.conv2d(inputs=conv4c_1x1redu5x5_S, filters=64, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4cM_1x1_S = tf.layers.conv2d(inputs=maxPool4c_3x3_S, filters=64, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      #####Concat5  intento\n",
        "      concat5 = tf.concat([conv4c_1x1_S, conv4c_3x3_S,conv4c_5x5_S ,conv4cM_1x1_S],axis=1)\n",
        "      \n",
        "      #####inception 4d###\n",
        "      conv4d_1x1redu3x3_S = tf.layers.conv2d(inputs=concat5, filters=144, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4d_1x1redu5x5_S = tf.layers.conv2d(inputs=concat5, filters=32, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool4d_3x3_S = tf.layers.max_pooling2d(inputs=concat5, pool_size=[3,3], strides=1)\n",
        "      \n",
        "      conv4d_1x1_S = tf.layers.conv2d(inputs=concat5, filters=112, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4d_3x3_S = tf.layers.conv2d(inputs=conv4d_1x1redu3x3_S, filters=288, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4d_5x5_S = tf.layers.conv2d(inputs=conv4d_1x1redu5x5_S, filters=64, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4dM_1x1_S = tf.layers.conv2d(inputs=maxPool4d_3x3_S, filters=64, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      ###Concat6 intento\n",
        "      \n",
        "      concat6 = tf.concat([conv4d_1x1_S, conv4d_3x3_S,conv4d_5x5_S ,conv4dM_1x1_S],axis=1)\n",
        "      \n",
        "      ##########inception 4e#####\n",
        "      conv4e_1x1redu3x3_S = tf.layers.conv2d(inputs=concat6, filters=160, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4e_1x1redu5x5_S = tf.layers.conv2d(inputs=concat6, filters=32, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool4e_3x3_S = tf.layers.max_pooling2d(inputs=concat6, pool_size=[3,3], strides=1)\n",
        "      averagePool4e_5x5_3V = tf.layers.average_pooling2d(inputs=concat3, pool_size=[5,5], strides=3)\n",
        "      \n",
        "      conv4e_1x1_S = tf.layers.conv2d(inputs=concat6, filters=256, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4e_3x3_S = tf.layers.conv2d(inputs=conv4e_1x1redu3x3_S, filters=320, kernel_size=[3,3],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4e_5x5_S = tf.layers.conv2d(inputs=conv4e_1x1redu5x5_S, filters=128, kernel_size=[5,5],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv4eM_1x1_S = tf.layers.conv2d(inputs=maxPool4e_3x3_S, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      conv4eAP_1x1_S = tf.layers.conv2d(inputs=averagePool4e_5x5_3V, filters=160, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      FC_4e1 = tf.layers.dense(inputs=conv4eAP_1x1_S, units=1024, activation=tf.nn.relu)\n",
        "      FC_4e2 = tf.layers.dense(inputs=FC_4e1, units=1000, activation=tf.nn.relu)\n",
        "      \n",
        "      \n",
        "      #################concat7#############################\n",
        "      print(\"todo bien hasta aquí 5\")\n",
        "      concat7 = tf.concat([conv4e_1x1_S, conv4e_3x3_S,conv4e_5x5_S ,conv4eM_1x1_S],axis=1)\n",
        "      \n",
        "      ####################Maxpool4\n",
        "      maxPool4_3x3_2S = tf.layers.max_pooling2d(inputs=concat7, pool_size=[3,3], strides=2)\n",
        "      \n",
        "      #############inception5a#############\n",
        "      conv5a_1x1redu3x3_S = tf.layers.conv2d(inputs=maxPool4_3x3_2S, filters=160, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv5a_1x1redu5x5_S = tf.layers.conv2d(inputs=maxPool4_3x3_2S, filters=32, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool5a_3x3_S = tf.layers.max_pooling2d(inputs=maxPool4_3x3_2S, pool_size=[3,3], strides=1)\n",
        "      \n",
        "      conv5a_1x1_S = tf.layers.conv2d(inputs=maxPool4_3x3_2S, filters=256, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv5a_3x3_S = tf.layers.conv2d(inputs=conv5a_1x1redu3x3_S, filters=320, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv5a_5x5_S = tf.layers.conv2d(inputs=conv5a_1x1redu5x5_S, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv5aM_1x1_S = tf.layers.conv2d(inputs=maxPool5a_3x3_S, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      \n",
        "      ###############concat8######################## intento\n",
        "      \n",
        "      concat8 = tf.concat([conv5a_1x1_S, conv5a_3x3_S,conv5a_5x5_S ,conv5aM_1x1_S],axis=1)\n",
        "      \n",
        "      \n",
        "      ###################inception5b\n",
        "      \n",
        "      conv5b_1x1redu3x3_S = tf.layers.conv2d(inputs=concat8, filters=192, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv5b_1x1redu5x5_S = tf.layers.conv2d(inputs=concat8, filters=48, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      maxPool5b_3x3_S = tf.layers.max_pooling2d(inputs=concat8, pool_size=[3,3], strides=1)\n",
        "      \n",
        "      conv5b_1x1_S = tf.layers.conv2d(inputs=concat8, filters=384, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv5b_3x3_S = tf.layers.conv2d(inputs=conv5b_1x1redu3x3_S, filters=384, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv5b_5x5_S = tf.layers.conv2d(inputs=conv5b_1x1redu5x5_S, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      conv5bM_1x1_S = tf.layers.conv2d(inputs=maxPool5b_3x3_S, filters=128, kernel_size=[1,1],strides=1, padding=\"same\", activation=tf.nn.relu)\n",
        "      \n",
        "      ####################concat9##############\n",
        "      print(\"todo bien hasta aquí 6 \")\n",
        "      \n",
        "      concat9 = tf.concat([conv5b_1x1_S, conv5b_3x3_S,conv5b_5x5_S ,conv5bM_1x1_S],axis=1)\n",
        "      \n",
        "      #########averagepoolF1\n",
        "      \n",
        "      averagePoolF1_7x7_V = tf.layers.average_pooling2d(inputs=concat9, pool_size=[7,7], strides=1)\n",
        "      \n",
        "      #############FCF\n",
        "      \n",
        "      FCF= tf.layers.dense(inputs=averagePoolF1_7x7_V, units=1024, activation=tf.nn.relu)\n",
        "      \n",
        " \n",
        "       \n",
        "      ##########Softmax final\n",
        "      softMax2= tf.keras.activations.softmax(FCF, axis=1)\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "      ##############################################################################################################\n",
        "\n",
        "      self._logits = tf.layers.dense(inputs=softMax2, units=10)\n",
        "\n",
        "#flaggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
        "\n",
        "      print(\"todo bien hasta aquí x2\")\n",
        "\n",
        "#flaggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
        "\n",
        "      #hola\n",
        "      \n",
        "      with tf.name_scope(\"loss_func\") as scope:\n",
        "\n",
        "        self._loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self._logits, labels=self._y_))\n",
        "        self._loss_val = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self._logits, labels=self._y_))\n",
        "\n",
        "        self._train_summary =tf.summary.scalar(\"loss_train\",self._loss)\n",
        "        self._val_summary =tf.summary.scalar(\"loss_val\",self._loss_val)\n",
        "\n",
        "      \n",
        "      \n",
        "      with tf.name_scope(\"optimizer\") as scope:\n",
        "\n",
        "        global_step = tf.Variable(0, trainable=False)\n",
        "        start_learning_rate = 1e-3\n",
        "        learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 1000, 0.9, staircase=True)\n",
        "\n",
        "        self._train_step = tf.train.AdamOptimizer(learning_rate).minimize(self._loss, global_step=global_step)\n",
        "\n",
        "        tf.summary.scalar(\"learning rate\", learning_rate)\n",
        "        tf.summary.scalar(\"global step\", global_step)\n",
        "\n",
        "      self._merged_summary_op = tf.summary.merge_all()\n",
        "      init = tf.global_variables_initializer()\n",
        "      self._saver = tf.train.Saver(max_to_keep=None)\n",
        "      \n",
        "      \n",
        "      \n",
        "    \n",
        "\n",
        "      gpu_options = tf.GPUOptions(allow_growth = True)\n",
        "\n",
        "      self._session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
        "\n",
        "      self._writer = tf.summary.FileWriter(\"./logs/cifar10\", self._session.graph)\n",
        "\n",
        "      self._session.run(init)\n",
        "    \n",
        " \n",
        "\n",
        "  def train(self, save_dir=os.getcwd(), batch_size=750):\n",
        "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "          \n",
        "\n",
        "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    dataset_train = dataset_train.shuffle(buffer_size=10000)\n",
        "    dataset_train = dataset_train.repeat()\n",
        "    dataset_train = dataset_train.batch(batch_size)\n",
        "    dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    dataset_test = dataset_test.repeat()\n",
        "    dataset_test = dataset_test.batch(batch_size)\n",
        "\n",
        "          \n",
        "\n",
        "    iter_train = dataset_train.make_one_shot_iterator()\n",
        "    iter_train_op = iter_train.get_next()\n",
        "    iter_test = dataset_test.make_one_shot_iterator()\n",
        "    iter_test_op = iter_test.get_next()\n",
        "\n",
        "    self.build_graph()\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(200):\n",
        "\n",
        "      batch_train = self._session.run([iter_train_op])\n",
        "      batch_x_train, batch_y_train = batch_train[0]\n",
        "\n",
        "      if i % 10 == 0:\n",
        "        batch_test = self._session.run([iter_test_op])\n",
        "        batch_x_test, batch_y_test = batch_test[0]\n",
        "\n",
        "        loss_train, summary_1 = self._session.run([self._loss, self._merged_summary_op], feed_dict={self._x_:batch_x_train, self._y_:batch_y_train, self._is_training:True})\n",
        "\n",
        "        loss_val, summary_2 = self._session.run([self._loss_val, self._val_summary], feed_dict={self._x_:batch_x_test, self._y_:batch_y_test, self._is_training:False})\n",
        "\n",
        "        print(\"Epoch: {0}, Loss Train: {1} Loss Val: {2}\". format(i, loss_train, loss_val))\n",
        "\n",
        "        self._writer.add_summary(summary_1, i)\n",
        "        self._writer.add_summary(summary_2, i)\n",
        "\n",
        "      self._train_step.run(session=self._session, feed_dict={self._x_:batch_x_train, self._y_:batch_y_train, self._is_training:True})\n",
        "\n",
        "      if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "        \n",
        "      checkpoint_path = os.path.join(save_dir, \"model\")\n",
        "      filename = self._saver.save(self._session, checkpoint_path)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  cnn=Train()\n",
        "  cnn.train\n",
        "\n",
        "cnn.train()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "todo bien hasta aquí 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1863\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1864\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1865\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 1 in both shapes must be equal, but are 3 and 1. Shapes are [?,3,3] and [?,1,1]. for 'model_12/concat1' (op: 'ConcatV2') with input shapes: [?,3,3,64], [?,3,3,128], [?,3,3,32], [?,1,1,32], [] and with computed input tensors: input[4] = <-1>.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f9804fab6005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-f9804fab6005>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, save_dir, batch_size)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0miter_test_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-f9804fab6005>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0;31m#print(conv3a_1x1_S.shape,conv3a_1x1_S.shape,conv3a_3x3_S.shape,conv3a_5x5_S.shape,conv3aM_1x1_S.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mconcat1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconv3a_1x1_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv3a_3x3_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv3a_5x5_S\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv3aM_1x1_S\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concat1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m       \u001b[0;31m#concat1 = tf.concat([conv3a_1x1_S,conv3a_3x3_S,conv3a_5x5_S],axis=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"todo bien hasta aquí 1.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1297\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1298\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1254\u001b[0m   \u001b[0m_attr_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 1256\u001b[0;31m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   1257\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m   \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    786\u001b[0m         op = g.create_op(op_type_name, inputs, dtypes=None, name=scope,\n\u001b[1;32m    787\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    789\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3614\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3615\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3616\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3617\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3618\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   2025\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   2026\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 2027\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimension 1 in both shapes must be equal, but are 3 and 1. Shapes are [?,3,3] and [?,1,1]. for 'model_12/concat1' (op: 'ConcatV2') with input shapes: [?,3,3,64], [?,3,3,128], [?,3,3,32], [?,1,1,32], [] and with computed input tensors: input[4] = <-1>."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEjsTONNGLY7",
        "colab_type": "code",
        "outputId": "356ec5fb-5c9e-4387-9cf9-e11e4173d213",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cnn._saver.save(cnn._session, 'my-model-test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my-model-test'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}